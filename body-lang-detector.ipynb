{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (0.10.14)\n",
      "Requirement already satisfied: opencv-python in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: pandas in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (1.4.2)\n",
      "Requirement already satisfied: absl-py in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: jaxlib in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: matplotlib in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (7.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: pycparser in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rosaleen/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716430041.349131 7368449 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 86), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1716430041.465451 7368581 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.476435 7368586 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.478439 7368584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.478448 7368582 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.478935 7368583 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.483025 7368587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.492621 7368582 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430041.495421 7368584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/rosaleen/Library/Python/3.9/lib/python/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-05-22 22:07:21.782 Python[70066:7368449] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# make detections based on recoloured feed\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# draw face landmarks\u001b[39;00m\n\u001b[1;32m     21\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, results\u001b[38;5;241m.\u001b[39mface_landmarks, mp_holistic\u001b[38;5;241m.\u001b[39mFACEMESH_TESSELATION)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # landmark detection models\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # make detections based on recoloured feed\n",
    "        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # draw face landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "\n",
    "        # draw right hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw left hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        # make detections\n",
    "        cv2.imshow('holistic model detections', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9594"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coords = len(results.face_landmarks.landmark) + len(results.pose_landmarks.landmark)\n",
    "landmarks = ['pose']\n",
    "for i in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(i), 'y{}'.format(i), 'z{}'.format(i), 'v{}'.format(i)]\n",
    "\n",
    "csv_writer = csv.writer(open('coords.csv', mode='w', newline=''), delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716430220.109989 7370631 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 86), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1716430220.253579 7371420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.266231 7371420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.269342 7371419 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.269336 7371420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.269891 7371414 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.274345 7371414 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.279092 7371420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430220.279292 7371419 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/rosaleen/Library/Python/3.9/lib/python/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-05-22 22:10:20.642 Python[70115:7370631] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# make detections based on recoloured feed\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# draw face landmarks\u001b[39;00m\n\u001b[1;32m     21\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, results\u001b[38;5;241m.\u001b[39mface_landmarks, mp_holistic\u001b[38;5;241m.\u001b[39mFACEMESH_TESSELATION)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # landmark detection models\n",
    "\n",
    "pose_name = \"emotionless\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # make detections based on recoloured feed\n",
    "        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # draw face landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "\n",
    "        # draw right hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw left hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        # export coordinates\n",
    "        try:\n",
    "            # extract pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # extract face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            csv_writer = csv.writer(open('coords.csv', mode='a', newline=''), delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerow([pose_name] + pose_row + face_row)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # make detections\n",
    "        cv2.imshow('holistic model detections', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('coords.csv')\n",
    "\n",
    "feat = df.drop('pose', axis=1) # all but first col\n",
    "targ = df['pose'] # first col\n",
    "\n",
    "feat_train, feat_test, targ_train, targ_test = train_test_split(feat, targ, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pose</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shock</td>\n",
       "      <td>0.625259</td>\n",
       "      <td>0.488210</td>\n",
       "      <td>-0.506665</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.655957</td>\n",
       "      <td>0.419267</td>\n",
       "      <td>-0.471772</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.670879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680587</td>\n",
       "      <td>0.427756</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686105</td>\n",
       "      <td>0.420578</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shock</td>\n",
       "      <td>0.617741</td>\n",
       "      <td>0.513095</td>\n",
       "      <td>-0.536126</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.650620</td>\n",
       "      <td>0.434174</td>\n",
       "      <td>-0.509820</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.667857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.671997</td>\n",
       "      <td>0.440529</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677512</td>\n",
       "      <td>0.433748</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shock</td>\n",
       "      <td>0.615368</td>\n",
       "      <td>0.530661</td>\n",
       "      <td>-0.532332</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.648318</td>\n",
       "      <td>0.446826</td>\n",
       "      <td>-0.503713</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.666445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.670444</td>\n",
       "      <td>0.442035</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675703</td>\n",
       "      <td>0.435932</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shock</td>\n",
       "      <td>0.612365</td>\n",
       "      <td>0.538415</td>\n",
       "      <td>-0.543973</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.644232</td>\n",
       "      <td>0.453737</td>\n",
       "      <td>-0.518162</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.669133</td>\n",
       "      <td>0.448209</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674554</td>\n",
       "      <td>0.440665</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shock</td>\n",
       "      <td>0.612730</td>\n",
       "      <td>0.540112</td>\n",
       "      <td>-0.540258</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.644534</td>\n",
       "      <td>0.455702</td>\n",
       "      <td>-0.512951</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.663492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.672703</td>\n",
       "      <td>0.454840</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.678015</td>\n",
       "      <td>0.446508</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pose        x1        y1        z1        v1        x2        y2  \\\n",
       "0  shock  0.625259  0.488210 -0.506665  0.999990  0.655957  0.419267   \n",
       "1  shock  0.617741  0.513095 -0.536126  0.999990  0.650620  0.434174   \n",
       "2  shock  0.615368  0.530661 -0.532332  0.999990  0.648318  0.446826   \n",
       "3  shock  0.612365  0.538415 -0.543973  0.999989  0.644232  0.453737   \n",
       "4  shock  0.612730  0.540112 -0.540258  0.999989  0.644534  0.455702   \n",
       "\n",
       "         z2        v2        x3  ...      z499  v499      x500      y500  \\\n",
       "0 -0.471772  0.999976  0.670879  ... -0.002376   0.0  0.680587  0.427756   \n",
       "1 -0.509820  0.999973  0.667857  ... -0.004143   0.0  0.671997  0.440529   \n",
       "2 -0.503713  0.999973  0.666445  ... -0.005700   0.0  0.670444  0.442035   \n",
       "3 -0.518162  0.999971  0.663392  ... -0.005395   0.0  0.669133  0.448209   \n",
       "4 -0.512951  0.999970  0.663492  ... -0.004440   0.0  0.672703  0.454840   \n",
       "\n",
       "       z500  v500      x501      y501      z501  v501  \n",
       "0  0.005928   0.0  0.686105  0.420578  0.005900   0.0  \n",
       "1  0.002907   0.0  0.677512  0.433748  0.002687   0.0  \n",
       "2  0.000257   0.0  0.675703  0.435932 -0.000181   0.0  \n",
       "3  0.000415   0.0  0.674554  0.440665  0.000153   0.0  \n",
       "4  0.002630   0.0  0.678015  0.446508  0.002567   0.0  \n",
       "\n",
       "[5 rows x 2005 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pose</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>emotionless</td>\n",
       "      <td>0.659273</td>\n",
       "      <td>0.605414</td>\n",
       "      <td>-0.993543</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.666334</td>\n",
       "      <td>0.536881</td>\n",
       "      <td>-0.950971</td>\n",
       "      <td>0.999188</td>\n",
       "      <td>0.676328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.036285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695221</td>\n",
       "      <td>0.512679</td>\n",
       "      <td>0.038389</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>emotionless</td>\n",
       "      <td>0.670889</td>\n",
       "      <td>0.607428</td>\n",
       "      <td>-1.085822</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.677520</td>\n",
       "      <td>0.537703</td>\n",
       "      <td>-1.043021</td>\n",
       "      <td>0.999231</td>\n",
       "      <td>0.687089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698708</td>\n",
       "      <td>0.517990</td>\n",
       "      <td>0.039551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700654</td>\n",
       "      <td>0.511061</td>\n",
       "      <td>0.041991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>emotionless</td>\n",
       "      <td>0.682681</td>\n",
       "      <td>0.607666</td>\n",
       "      <td>-0.982500</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.687199</td>\n",
       "      <td>0.539192</td>\n",
       "      <td>-0.941686</td>\n",
       "      <td>0.999198</td>\n",
       "      <td>0.694559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706535</td>\n",
       "      <td>0.514982</td>\n",
       "      <td>0.041727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708358</td>\n",
       "      <td>0.508194</td>\n",
       "      <td>0.044360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>emotionless</td>\n",
       "      <td>0.690894</td>\n",
       "      <td>0.607655</td>\n",
       "      <td>-0.942443</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.692222</td>\n",
       "      <td>0.539315</td>\n",
       "      <td>-0.904090</td>\n",
       "      <td>0.999097</td>\n",
       "      <td>0.698523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711501</td>\n",
       "      <td>0.518049</td>\n",
       "      <td>0.042844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713171</td>\n",
       "      <td>0.511160</td>\n",
       "      <td>0.045578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>emotionless</td>\n",
       "      <td>0.697242</td>\n",
       "      <td>0.607701</td>\n",
       "      <td>-0.866478</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>0.698804</td>\n",
       "      <td>0.539647</td>\n",
       "      <td>-0.831656</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.704857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714658</td>\n",
       "      <td>0.515532</td>\n",
       "      <td>0.044254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716132</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pose        x1        y1        z1        v1        x2        y2  \\\n",
       "523  emotionless  0.659273  0.605414 -0.993543  0.999724  0.666334  0.536881   \n",
       "524  emotionless  0.670889  0.607428 -1.085822  0.999735  0.677520  0.537703   \n",
       "525  emotionless  0.682681  0.607666 -0.982500  0.999712  0.687199  0.539192   \n",
       "526  emotionless  0.690894  0.607655 -0.942443  0.999657  0.692222  0.539315   \n",
       "527  emotionless  0.697242  0.607701 -0.866478  0.999627  0.698804  0.539647   \n",
       "\n",
       "           z2        v2        x3  ...      z499  v499      x500      y500  \\\n",
       "523 -0.950971  0.999188  0.676328  ...  0.002029   0.0  0.692779  0.519000   \n",
       "524 -1.043021  0.999231  0.687089  ...  0.002399   0.0  0.698708  0.517990   \n",
       "525 -0.941686  0.999198  0.694559  ...  0.002003   0.0  0.706535  0.514982   \n",
       "526 -0.904090  0.999097  0.698523  ...  0.002352   0.0  0.711501  0.518049   \n",
       "527 -0.831656  0.999050  0.704857  ...  0.002903   0.0  0.714658  0.515532   \n",
       "\n",
       "         z500  v500      x501      y501      z501  v501  \n",
       "523  0.036285   0.0  0.695221  0.512679  0.038389   0.0  \n",
       "524  0.039551   0.0  0.700654  0.511061  0.041991   0.0  \n",
       "525  0.041727   0.0  0.708358  0.508194  0.044360   0.0  \n",
       "526  0.042844   0.0  0.713171  0.511160  0.045578   0.0  \n",
       "527  0.044254   0.0  0.716132  0.508685  0.047074   0.0  \n",
       "\n",
       "[5 rows x 2005 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline # to make ML pipeline\n",
    "from sklearn.preprocessing import StandardScaler # to standardize data\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc': make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}\n",
    "\n",
    "fit_models= {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    fit_models[algo] = pipeline.fit(feat_train, targ_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['shock', 'emotionless', 'shock', 'shock', 'emotionless', 'shock',\n",
       "       'shock', 'shock', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'shock', 'shock', 'emotionless',\n",
       "       'shock', 'shock', 'shock', 'emotionless', 'shock', 'emotionless',\n",
       "       'shock', 'shock', 'emotionless', 'shock', 'shock', 'shock',\n",
       "       'shock', 'emotionless', 'shock', 'emotionless', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'shock', 'emotionless', 'shock',\n",
       "       'emotionless', 'shock', 'emotionless', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'emotionless', 'shock', 'shock', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'emotionless', 'emotionless',\n",
       "       'shock', 'emotionless', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'emotionless', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'emotionless',\n",
       "       'emotionless', 'emotionless', 'emotionless', 'shock', 'shock',\n",
       "       'shock', 'emotionless', 'shock', 'shock', 'shock', 'emotionless',\n",
       "       'shock', 'shock', 'shock', 'shock', 'shock', 'emotionless',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'emotionless', 'shock', 'emotionless', 'shock', 'shock',\n",
       "       'shock', 'emotionless', 'shock', 'shock', 'shock', 'shock',\n",
       "       'shock', 'shock', 'shock', 'emotionless', 'shock', 'shock',\n",
       "       'emotionless', 'shock', 'shock', 'shock', 'shock', 'shock',\n",
       "       'emotionless', 'emotionless'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rf'].predict(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 1.0\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # accuracy metrics\n",
    "import pickle\n",
    "\n",
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(feat_test)\n",
    "    print(algo, accuracy_score(targ_test, yhat)) # print accuracy\n",
    "\n",
    "pickle.dump(fit_models['rf'], open('body_language.pkl', 'wb'))\n",
    "model = pickle.load(open('body_language.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1716430559.630812 7375194 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 86), renderer: Apple M2\n",
      "W0000 00:00:1716430559.701982 7375434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.711116 7375431 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.712237 7375432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.712245 7375433 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.712662 7375435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.718882 7375435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.721346 7375432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716430559.733053 7375433 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-05-22 22:15:59.914 Python[70169:7375194] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# make detections based on recoloured feed\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m holistic\u001b[38;5;241m.\u001b[39mprocess(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# draw face landmarks\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, results\u001b[38;5;241m.\u001b[39mface_landmarks, mp_holistic\u001b[38;5;241m.\u001b[39mFACEMESH_TESSELATION)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # landmark detection models\n",
    "\n",
    "model = pickle.load(open('body_language.pkl', 'rb'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # make detections based on recoloured feed\n",
    "        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # draw face landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "\n",
    "        # draw right hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw left hand landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "        # export coordinates\n",
    "        try:\n",
    "            # extract pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "            # extract face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            row = pose_row + face_row\n",
    "\n",
    "            feat = pd.DataFrame([row])\n",
    "            bl_class = model.predict(feat)[0]\n",
    "            bl_prob = model.predict_proba(feat)[0]\n",
    "\n",
    "            # get status box\n",
    "            cv2.rectangle(frame, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "            cv2.putText(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 'CLASS'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (234,244,232), 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame, bl_class.split(' ')[0]\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (190,216,184), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "            cv2.putText(frame, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (234,244,232), 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame, str(round(bl_prob[np.argmax(bl_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (190,216,184), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # make detections\n",
    "        cv2.imshow('holistic model detections', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
