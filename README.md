# body-ody-ody-ody
facial emotion and body language detector ğŸ˜™ğŸ˜ğŸ˜ğŸ¤¨

# goal
the aim of this project is to:
1. capture real-time footage from device camera,
2. pinpoint the landmarks on the face and body,
3. detect the emotion of the user.

# implementation
you ask: "how does this project work?"
your great question is compounded by my thorough answer: 
1. "capture real-time footage from the device camera using cv2,
2. capture face, body, and finger landmarks using mediapipe holistics,
3. make silly faces and create a dataset based of such,
4. train the dataset model using sklearn,
5. re-open the device camera, make silly faces, and watch as the application detects your emotion and body language! ğŸ˜±ğŸ˜ğŸ˜

# applicability of this project
this code has provided me the ability to gain a deeper understanding of facial / body detection using media pipe -- i hope to leverage this knowledge to create an application that can detect ASL hand motions, effectively translating the actions to english! :)
